{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgozon/DLG-UROP/blob/main/Batch_DLG_Iris_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch-DLG - Iris Dataset\n",
        "This notebook modifies the code in [Deep Leakage from Gradients](https://gist.github.com/Lyken17/91b81526a8245a028d4f85ccc9191884) to work with the Iris Dataset. In addition, it explores whether it is possible to repeat the same procedure on the batch input gradient."
      ],
      "metadata": {
        "id": "EFXvpWu88EO4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWa7Xo6PkIl3",
        "outputId": "c5120796-98fa-4754-b1ca-c10f76884b49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# setting up libraries and device\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import grad\n",
        "\n",
        "from random import randint\n",
        "from random import shuffle\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "dst = load_iris()\n",
        "\n",
        "print(torch.__version__)\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "print(\"Running on %s\" % device)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n",
            "Running on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjKWqs2akepH"
      },
      "source": [
        "# auxiliary functions for NN - conver to onehot and loss function\n",
        "def label_to_onehot(target, num_classes = 3):\n",
        "    target = torch.unsqueeze(target, 1)\n",
        "    onehot_target = torch.zeros(target.size(0), num_classes, device=target.device)\n",
        "    onehot_target.scatter_(1, target, 1)\n",
        "    return onehot_target\n",
        "\n",
        "def cross_entropy_for_onehot(pred, target):\n",
        "    return torch.mean(torch.sum(- target * F.log_softmax(pred, dim=-1), 1))"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AorI020iVjjS"
      },
      "source": [
        "# a random fully connected neural network with random weights and biases\n",
        "def weights_init(m):\n",
        "    if hasattr(m, \"weight\"):\n",
        "        m.weight.data.uniform_(-0.5, 0.5)\n",
        "    if hasattr(m, \"bias\"):\n",
        "        m.bias.data.uniform_(-0.5, 0.5)\n",
        "    \n",
        "class FcNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FcNet, self).__init__()\n",
        "        act = nn.Sigmoid\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Linear(4, 100),\n",
        "            act(),\n",
        "            nn.Linear(100, 100),\n",
        "            act(),\n",
        "            nn.Linear(100, 100),\n",
        "            act(),\n",
        "            nn.Linear(100, 3),\n",
        "            act(),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.body(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return out\n",
        "    \n",
        "net = FcNet().to(device)\n",
        "    \n",
        "net.apply(weights_init)\n",
        "criterion = cross_entropy_for_onehot"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mSgR4GClV-8"
      },
      "source": [
        "# DLG algorithm on a given flower and returns the hypothesized input\n",
        "def Batch_DLG(flower_indices, verbose = 0):\n",
        "    n = len(flower_indices)\n",
        "\n",
        "    gt_dataset = []\n",
        "    gt_labels = []\n",
        "    for flower_index in flower_indices:\n",
        "        gt_data = torch.tensor(dst.data[flower_index, :]).to(device)\n",
        "        gt_data = gt_data.view(1, *gt_data.size())\n",
        "        gt_dataset.append(gt_data)\n",
        "        gt_label = torch.tensor(dst.target[flower_index]).to(device)\n",
        "        gt_label = gt_label.view(1)\n",
        "        gt_labels.append(gt_label)\n",
        "        gt_onehot_label = label_to_onehot(gt_label, num_classes=3)\n",
        "\n",
        "        # print out (data, label) and verify onehot\n",
        "        if (verbose == 2):\n",
        "            print(f\"gt_data: {gt_data}\")\n",
        "            print(f\"gt_label: {gt_label}\")\n",
        "            print(f\"gt_onehot_label: {gt_onehot_label}\")\n",
        "            print(f\"flower {flower_index} has label (gt, onehot) = ({gt_label.item()}, {torch.argmax(gt_onehot_label, dim=-1).item()})\")\n",
        "\n",
        "        # compute original gradient \n",
        "        out = net(gt_data.float())\n",
        "        y = criterion(out, gt_onehot_label)\n",
        "\n",
        "        if (flower_index == flower_indices[0]):\n",
        "          batch_dy_dx = torch.autograd.grad(y, net.parameters())\n",
        "        else:\n",
        "          batch_dy_dx = tuple(map(sum, zip(batch_dy_dx, torch.autograd.grad(y, net.parameters()))))\n",
        "\n",
        "    #print(batch_dy_dx)\n",
        "    batch_dy_dx = tuple(part/n for part in batch_dy_dx)\n",
        "    # share the gradients with other clients\n",
        "    original_dy_dx = list((_.detach().clone() for _ in batch_dy_dx))\n",
        "\n",
        "    # generate dummy data and label\n",
        "    dummy_data = [torch.randn(gt_data.size()).to(device).requires_grad_(True) for i in range(n)]\n",
        "    dummy_label = [torch.randn(gt_onehot_label.size()).to(device).requires_grad_(True) for i in range(n)]\n",
        "\n",
        "    # if (verbose):\n",
        "    #     print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n",
        "\n",
        "    # identify (data, label) using LBFGS on the squared difference between the original and guessed gradient\n",
        "    \n",
        "\n",
        "    global opt_steps\n",
        "    opt_steps = 0\n",
        "    for iters in range(10):\n",
        "        idx = iters % n\n",
        "        optimizer = torch.optim.LBFGS([dummy_data[idx], dummy_label[idx]]) # only update one dummy variable at a time\n",
        "        for i in range(n):\n",
        "            if (i != idx):\n",
        "                dummy_data[i].requires_grad_(False)\n",
        "                dummy_label[i].requires_grad_(False)\n",
        "            else:\n",
        "                dummy_data[i].requires_grad_(True)\n",
        "                dummy_label[i].requires_grad_(True)\n",
        "\n",
        "        def closure():\n",
        "            global opt_steps\n",
        "            opt_steps += 1\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # compute loss\n",
        "            for i in range(n):\n",
        "                pred = net(dummy_data[i]) \n",
        "                #print(f\"prediction: {pred} from data: {dummy_data.data} and label: {dummy_label}\") # uncomment to see optimization updates\n",
        "                dummy_onehot_label = F.softmax(dummy_label[i], dim=-1)\n",
        "                dummy_loss = criterion(pred, dummy_onehot_label) if (i == 0) else dummy_loss + criterion(pred, dummy_onehot_label)\n",
        "            \n",
        "            dummy_loss /= n\n",
        "\n",
        "            dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
        "            \n",
        "            grad_diff = 0\n",
        "            grad_count = 0\n",
        "            for gx, gy in zip(dummy_dy_dx, original_dy_dx):\n",
        "                grad_diff += ((gx - gy) ** 2).sum()\n",
        "                grad_count += gx.nelement()\n",
        "\n",
        "            grad_diff.backward()\n",
        "            \n",
        "            return grad_diff\n",
        "        \n",
        "        optimizer.step(closure)\n",
        "        current_loss = closure()\n",
        "        if (verbose == 2):\n",
        "            print(iters, \"%.4f\" % current_loss.item())\n",
        "            print('dummy data: ', [dummy_data[i].tolist() for i in range(n)])\n",
        "            print('dummy labels: ', [dummy_label[i].tolist() for i in range(n)])\n",
        "        \n",
        "        # if current_loss is small enough, then the model has 'converged'\n",
        "        if (current_loss < 1e-9):\n",
        "            break\n",
        "    \n",
        "    # compare results\n",
        "    if (verbose):\n",
        "        print(f\"Original data: {gt_dataset}\")\n",
        "        print(f\"Predicted data: {dummy_data}\")\n",
        "        print(f\"Original label: {gt_labels}\")\n",
        "        #print(f\"Predicted label: {torch.argmax(dummy_label).item()}\")\n",
        "        #print(f\"Label SE: {((gt_data - dummy_data)**2).sum()}\")\n",
        "    \n",
        "    return dummy_data, sum([torch.sum((gt_dataset[i] - dummy_data[i])**2).item() for i in range(n)]), opt_steps"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aokP-jhal96-",
        "outputId": "11775154-f030-4b97-eab7-ec8b79459b8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# testing Batch-DLG on n random flowers\n",
        "length = dst.data.shape[0]\n",
        "perm = list(range(length))\n",
        "shuffle(perm)\n",
        "n = 1\n",
        "print('flowers: ', perm[0:n])\n",
        "\n",
        "guess, SE, steps = Batch_DLG(perm[0:n], verbose=1)\n",
        "print('guess: ', [guess[i].tolist() for i in range(n)])\n",
        "print('SE: ', SE)\n",
        "print('steps: ', steps)\n"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flowers:  [47]\n",
            "Original data: [tensor([[4.6000, 3.2000, 1.4000, 0.2000]], device='cuda:0',\n",
            "       dtype=torch.float64)]\n",
            "Predicted data: [tensor([[-18.4930,   5.9030,  17.4342, -48.2738]], device='cuda:0',\n",
            "       requires_grad=True)]\n",
            "Original label: [tensor([0], device='cuda:0')]\n",
            "guess:  [[[-18.49297332763672, 5.903026103973389, 17.43424415588379, -48.27377700805664]]]\n",
            "SE:  3147.395810306997\n",
            "steps:  103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QRMMm3Y5QNkq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}