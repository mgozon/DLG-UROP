{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgozon/DLG-UROP/blob/main/Batch_DLG_Iris_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch-DLG - Iris Dataset\n",
        "This notebook modifies the code in [Deep Leakage from Gradients](https://gist.github.com/Lyken17/91b81526a8245a028d4f85ccc9191884) to work with the Iris Dataset. In addition, it explores whether it is possible to repeat the same procedure on the batch input gradient."
      ],
      "metadata": {
        "id": "EFXvpWu88EO4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWa7Xo6PkIl3",
        "outputId": "aaa50a9e-262d-405c-8189-ed8b9b8c9866",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# setting up libraries and device\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import grad\n",
        "torch.manual_seed(100) # for generating the same random weights\n",
        "\n",
        "# for testing\n",
        "from random import randint\n",
        "from random import shuffle\n",
        "from itertools import permutations\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "dst = load_iris()\n",
        "\n",
        "print(torch.__version__)\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "print(\"Running on %s\" % device)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n",
            "Running on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjKWqs2akepH"
      },
      "source": [
        "# auxiliary functions for NN - conver to onehot and loss function\n",
        "def label_to_onehot(target, num_classes = 3):\n",
        "    target = torch.unsqueeze(target, 1)\n",
        "    onehot_target = torch.zeros(target.size(0), num_classes, device=target.device)\n",
        "    onehot_target.scatter_(1, target, 1)\n",
        "    return onehot_target\n",
        "\n",
        "def cross_entropy_for_onehot(pred, target):\n",
        "    return torch.mean(torch.sum(- target * F.log_softmax(pred, dim=-1), 1))"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AorI020iVjjS"
      },
      "source": [
        "# a random fully connected neural network with random weights and biases\n",
        "def weights_init(m):\n",
        "    if hasattr(m, \"weight\"):\n",
        "        m.weight.data.uniform_(-0.5, 0.5)\n",
        "    if hasattr(m, \"bias\"):\n",
        "        m.bias.data.uniform_(-0.5, 0.5)\n",
        "    \n",
        "class FcNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FcNet, self).__init__()\n",
        "        act = nn.Sigmoid\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Linear(4, 100),\n",
        "            act(),\n",
        "            nn.Linear(100, 100),\n",
        "            act(),\n",
        "            nn.Linear(100, 100),\n",
        "            act(),\n",
        "            nn.Linear(100, 3),\n",
        "            act(),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.body(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return out\n",
        "    \n",
        "net = FcNet().to(device)\n",
        "    \n",
        "net.apply(weights_init)\n",
        "criterion = cross_entropy_for_onehot"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# process input flowers and compute gradient of batch\n",
        "def batch_grad(flower_indices, verbose = 0):\n",
        "    n = len(flower_indices)\n",
        "\n",
        "    gt_dataset = []\n",
        "    gt_labels = []\n",
        "    for flower_index in flower_indices:\n",
        "        gt_data = torch.tensor(dst.data[flower_index, :]).to(device)\n",
        "        gt_data = gt_data.view(1, *gt_data.size())\n",
        "        gt_dataset.append(gt_data)\n",
        "        gt_label = torch.tensor(dst.target[flower_index]).to(device)\n",
        "        gt_label = gt_label.view(1)\n",
        "        gt_labels.append(gt_label)\n",
        "        gt_onehot_label = label_to_onehot(gt_label, num_classes=3)\n",
        "\n",
        "        # print out (data, label) and verify onehot\n",
        "        if (verbose):\n",
        "            print(f\"gt_data: {gt_data}\")\n",
        "            print(f\"gt_label: {gt_label}\")\n",
        "            print(f\"gt_onehot_label: {gt_onehot_label}\")\n",
        "            print(f\"flower {flower_index} has label (gt, onehot) = ({gt_label.item()}, {torch.argmax(gt_onehot_label, dim=-1).item()})\")\n",
        "\n",
        "        # compute original gradient \n",
        "        out = net(gt_data.float())\n",
        "        y = criterion(out, gt_onehot_label)\n",
        "\n",
        "        if (flower_index == flower_indices[0]):\n",
        "          batch_dy_dx = torch.autograd.grad(y, net.parameters())\n",
        "        else:\n",
        "          batch_dy_dx = tuple(map(sum, zip(batch_dy_dx, torch.autograd.grad(y, net.parameters())))) # sum of gradients\n",
        "\n",
        "    batch_dy_dx = tuple(part/n for part in batch_dy_dx)\n",
        "    original_dy_dx = list((_.detach().clone() for _ in batch_dy_dx)) # share the gradients with other clients\n",
        "\n",
        "    # verifying dy_dx is average of list of flowers\n",
        "    if (verbose >= 2):\n",
        "      print(original_dy_dx)\n",
        "    \n",
        "    return original_dy_dx, gt_dataset, gt_labels"
      ],
      "metadata": {
        "id": "GWL_MjKJIZ9C"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mSgR4GClV-8"
      },
      "source": [
        "# DLG algorithm on a given set of flowers and returns the hypothesized input\n",
        "def batch_DLG(original_dy_dx, n, gt_data_len, gt_onehot_label_len, verbose = 0):\n",
        "\n",
        "    # identify (data, label) using Adam on the squared difference between the original and guessed gradient\n",
        "    dummy_data = [torch.randn(gt_data_len).to(device).requires_grad_(True) for i in range(n)]\n",
        "    dummy_label = [torch.randn(gt_onehot_label_len).to(device).requires_grad_(True) for i in range(n)]\n",
        "    optimizer = torch.optim.LBFGS(dummy_data+dummy_label)\n",
        "\n",
        "    global opt_steps; opt_steps = 0\n",
        "    for iters in range(100):\n",
        "\n",
        "        # closure function needed for LBFGS optimizer\n",
        "        def closure():\n",
        "            global opt_steps; opt_steps += 1\n",
        "\n",
        "            # compute gradient of dummy data/label\n",
        "            optimizer.zero_grad()\n",
        "            for i in range(n):\n",
        "                pred = net(dummy_data[i]) \n",
        "                #print(f\"prediction: {pred} from data: {dummy_data.data} and label: {dummy_label}\") # uncomment to see optimization updates\n",
        "                dummy_onehot_label = F.softmax(dummy_label[i], dim=-1)\n",
        "                dummy_loss = criterion(pred, dummy_onehot_label) if (i == 0) else dummy_loss + criterion(pred, dummy_onehot_label)\n",
        "            \n",
        "            dummy_loss /= n\n",
        "            dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
        "            \n",
        "            # compute loss function, i.e. the SE of the gradients\n",
        "            grad_diff = 0\n",
        "            grad_count = 0\n",
        "            for gx, gy in zip(dummy_dy_dx, original_dy_dx):\n",
        "                grad_diff += ((gx - gy) ** 2).sum()\n",
        "                grad_count += gx.nelement()\n",
        "\n",
        "            grad_diff.backward()\n",
        "            return grad_diff\n",
        "        \n",
        "        # perform GD and log information\n",
        "        optimizer.step(closure)\n",
        "        current_loss = closure()\n",
        "        if (verbose == 2):\n",
        "            print('current loss: ', iters, \"%.4f\" % current_loss.item())\n",
        "            print('dummy data: ', dummy_data)\n",
        "            print('dummy labels: ', dummy_label)\n",
        "        \n",
        "        # if current_loss is small enough, then the model has 'converged'\n",
        "        if (closure() < 1e-9):\n",
        "            break\n",
        "    \n",
        "    return dummy_data, opt_steps"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify batch-DLG manually\n",
        "flower_indices = [87, 67, 34, 12, 100]\n",
        "n = len(flower_indices)\n",
        "original_dy_dx, gt_dataset, gt_labels = batch_grad(flower_indices, 0)\n",
        "guess, steps = batch_DLG(original_dy_dx, n=len(flower_indices), gt_data_len=gt_dataset[0].size(), gt_onehot_label_len=label_to_onehot(gt_labels[0]).size(), verbose=1)\n",
        "SE = sum([torch.sum((gt_dataset[i] - guess[i])**2).item() for i in range(n)])\n",
        "print(f\"Original data: {gt_dataset}\")\n",
        "print(f\"Predicted data: {guess}\")\n",
        "print('(naive assignment SE, steps) = ', SE, steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPXhp3rpCAC8",
        "outputId": "cfb9c240-d7b3-4a76-fe48-efc1d2773595"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data: [tensor([[6.3000, 2.3000, 4.4000, 1.3000]], device='cuda:0',\n",
            "       dtype=torch.float64), tensor([[5.8000, 2.7000, 4.1000, 1.0000]], device='cuda:0',\n",
            "       dtype=torch.float64), tensor([[4.9000, 3.1000, 1.5000, 0.2000]], device='cuda:0',\n",
            "       dtype=torch.float64), tensor([[4.8000, 3.0000, 1.4000, 0.1000]], device='cuda:0',\n",
            "       dtype=torch.float64), tensor([[6.3000, 3.3000, 6.0000, 2.5000]], device='cuda:0',\n",
            "       dtype=torch.float64)]\n",
            "Predicted data: [tensor([[5.8422, 2.6845, 4.1353, 0.9543]], device='cuda:0', requires_grad=True), tensor([[4.7462, 2.9267, 1.3718, 0.0624]], device='cuda:0', requires_grad=True), tensor([[6.2708, 2.3105, 4.3732, 1.3411]], device='cuda:0', requires_grad=True), tensor([[6.3229, 3.3103, 6.0374, 2.5194]], device='cuda:0', requires_grad=True), tensor([[4.9613, 3.1803, 1.5144, 0.2287]], device='cuda:0', requires_grad=True)]\n",
            "(naive assignment SE, steps) =  78.95178247574894 432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aokP-jhal96-",
        "outputId": "d3b3fbd5-5a35-4162-dfbf-dd769c1f3780",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# testing Batch-DLG on n random flowers\n",
        "length = dst.data.shape[0]\n",
        "perm = list(range(length))\n",
        "shuffle(perm)\n",
        "n = 8\n",
        "flower_indices = perm[0:n]\n",
        "print('flowers: ', flower_indices)\n",
        "\n",
        "original_dy_dx, gt_dataset, gt_labels = batch_grad(flower_indices, 0)\n",
        "guess, steps = batch_DLG(original_dy_dx, n, gt_dataset[0].size(), label_to_onehot(gt_labels[0]).size(), 1)\n",
        "print(f\"Original data: {gt_dataset}\")\n",
        "print(f\"Predicted data: {guess}\")\n",
        "print('steps: ', steps)\n",
        "\n",
        "# brute force best assignment - if larger n is needed i.e. n >= 9, then use network flow\n",
        "best_perm = tuple(range(0, n))\n",
        "best_SE = sum([torch.sum((gt_dataset[i] - guess[i])**2).item() for i in range(n)])\n",
        "\n",
        "for permutation in permutations(range(0, n)):\n",
        "    SE = 0\n",
        "    for i in range(n):\n",
        "        SE += torch.sum((gt_dataset[i] - guess[permutation[i]])**2).item()\n",
        "    if (SE < best_SE):\n",
        "        best_SE = SE\n",
        "        best_perm = permutation\n",
        "\n",
        "print('best guessed to real data assignment: ', best_perm)\n",
        "print('best SE: ', best_SE)\n",
        "guess_perm = [None] * n\n",
        "for i in range(n):\n",
        "    guess_perm[i] = guess[best_perm[i]]\n",
        "print('guessed permutation: ', guess_perm)\n",
        "\n",
        "print('side by side comparison of guessed to actual input data: ')\n",
        "for i in range(n):\n",
        "    print(i, ':', gt_dataset[i].tolist(), guess_perm[i].tolist())"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flowers:  [135, 41, 136, 145, 99, 7, 46, 129]\n",
            "Original data: [tensor([[7.7000, 3.0000, 6.1000, 2.3000]], device='cuda:0',\n",
            "       dtype=torch.float64), tensor([[4.5000, 2.3000, 1.3000, 0.3000]], device='cuda:0',\n",
            "       dtype=torch.float64), tensor([[6.3000, 3.4000, 5.6000, 2.4000]], device='cuda:0',\n",
            "       dtype=torch.float64), tensor([[6.7000, 3.0000, 5.2000, 2.3000]], device='cuda:0',\n",
            "       dtype=torch.float64), tensor([[5.7000, 2.8000, 4.1000, 1.3000]], device='cuda:0',\n",
            "       dtype=torch.float64), tensor([[5.0000, 3.4000, 1.5000, 0.2000]], device='cuda:0',\n",
            "       dtype=torch.float64), tensor([[5.1000, 3.8000, 1.6000, 0.2000]], device='cuda:0',\n",
            "       dtype=torch.float64), tensor([[7.2000, 3.0000, 5.8000, 1.6000]], device='cuda:0',\n",
            "       dtype=torch.float64)]\n",
            "Predicted data: [tensor([[5.1312, 3.8606, 1.5707, 0.3258]], device='cuda:0', requires_grad=True), tensor([[7.6203, 3.1238, 5.8060, 2.3110]], device='cuda:0', requires_grad=True), tensor([[4.9940, 3.3334, 1.5347, 0.0330]], device='cuda:0', requires_grad=True), tensor([[7.1831, 3.2230, 6.0174, 1.7679]], device='cuda:0', requires_grad=True), tensor([[6.9523, 2.6962, 5.5955, 2.0237]], device='cuda:0', requires_grad=True), tensor([[5.7644, 2.8283, 4.2281, 1.3513]], device='cuda:0', requires_grad=True), tensor([[4.4462, 2.2989, 1.1874, 0.3060]], device='cuda:0', requires_grad=True), tensor([[6.2254, 3.4088, 5.4304, 2.5863]], device='cuda:0', requires_grad=True)]\n",
            "steps:  465\n",
            "best guessed to real data assignment:  (1, 6, 7, 4, 5, 2, 0, 3)\n",
            "best SE:  0.7860418118374235\n",
            "guessed permutation:  [tensor([[7.6203, 3.1238, 5.8060, 2.3110]], device='cuda:0', requires_grad=True), tensor([[4.4462, 2.2989, 1.1874, 0.3060]], device='cuda:0', requires_grad=True), tensor([[6.2254, 3.4088, 5.4304, 2.5863]], device='cuda:0', requires_grad=True), tensor([[6.9523, 2.6962, 5.5955, 2.0237]], device='cuda:0', requires_grad=True), tensor([[5.7644, 2.8283, 4.2281, 1.3513]], device='cuda:0', requires_grad=True), tensor([[4.9940, 3.3334, 1.5347, 0.0330]], device='cuda:0', requires_grad=True), tensor([[5.1312, 3.8606, 1.5707, 0.3258]], device='cuda:0', requires_grad=True), tensor([[7.1831, 3.2230, 6.0174, 1.7679]], device='cuda:0', requires_grad=True)]\n",
            "side by side comparison of guessed to actual input data: \n",
            "0 : [[7.7, 3.0, 6.1, 2.3]] [[7.620316982269287, 3.123847007751465, 5.805971622467041, 2.3109965324401855]]\n",
            "1 : [[4.5, 2.3, 1.3, 0.3]] [[4.446201324462891, 2.298924446105957, 1.1873517036437988, 0.30597278475761414]]\n",
            "2 : [[6.3, 3.4, 5.6, 2.4]] [[6.225353240966797, 3.4087607860565186, 5.430376052856445, 2.586282730102539]]\n",
            "3 : [[6.7, 3.0, 5.2, 2.3]] [[6.952291965484619, 2.696188449859619, 5.595466613769531, 2.0237386226654053]]\n",
            "4 : [[5.7, 2.8, 4.1, 1.3]] [[5.764434814453125, 2.8283121585845947, 4.228143692016602, 1.3512928485870361]]\n",
            "5 : [[5.0, 3.4, 1.5, 0.2]] [[4.99397611618042, 3.3333520889282227, 1.5347295999526978, 0.032957516610622406]]\n",
            "6 : [[5.1, 3.8, 1.6, 0.2]] [[5.131176471710205, 3.8605735301971436, 1.5707151889801025, 0.3258405327796936]]\n",
            "7 : [[7.2, 3.0, 5.8, 1.6]] [[7.183101177215576, 3.2229535579681396, 6.0174055099487305, 1.7678818702697754]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gLZEAjIwTnUb"
      },
      "execution_count": 75,
      "outputs": []
    }
  ]
}