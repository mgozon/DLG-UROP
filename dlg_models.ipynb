{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPuGTyxfhJFLnIByh/u7OG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgozon/DLG-UROP/blob/main/dlg_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DLG Models\n",
        "This notebook provides models with and without DLG.\n",
        "- batch_DLG_Adam\n",
        "- label_to_onehot\n",
        "- train_net_Adam\n",
        "- test_net\n",
        "- train_net_Adam_DLG\n",
        "\n",
        "dlg_stats is also imported for analysis in training with dlg (future: consider removing dependency and doing all analysis in another notebook)"
      ],
      "metadata": {
        "id": "d1td7mVGBbJ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV2_Qjan87w2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import grad\n",
        "from torch.utils.data import RandomSampler                                        # sample random minibatch\n",
        "from tqdm.notebook import trange\n",
        "from math import sqrt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Adding dlg_stats\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%run 'drive/MyDrive/UROP/Project Files/dlg_stats.ipynb'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz4vkKn-RDAN",
        "outputId": "08a102c4-4d33-4b73-9cda-dc84a0162189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "defined: assign_guess(guess, gt_dataset, n, verbose = False): guess_perm\n",
            "defined: assign_best(guess, gt_dataset, n, verbose = False): best_match\n",
            "defined: compute_stats(guess_perm, gt_data, recovered_threshold = 0.25): rel_errors, recovered_rate, cos_angles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DLG Algorithm"
      ],
      "metadata": {
        "id": "lmO6HTKPcIGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Batch-DLG with LBFGS (not updated)\n",
        "# ***NOTE - this gives NaNs when ReLU is used since it requires a differentiable loss function\n",
        "# --> also possibly due to PyTorch implementation error - https://github.com/pytorch/pytorch/issues/5953)\n",
        "\n",
        "# DLG algorithm on a given set of flowers and returns the hypothesized input\n",
        "def batch_DLG_LBFGS(original_dy_dx, batch_size, gt_data_len, gt_onehot_label_len, verbose = False):\n",
        "    losses = []\n",
        "\n",
        "    # identify (data, label) using LBFGS on the squared difference between the original and guessed gradient\n",
        "    dummy_data = torch.randn(batch_size, gt_data_len).to(device).requires_grad_(True)\n",
        "    dummy_label = torch.randn(batch_size, gt_onehot_label_len).to(device).requires_grad_(True)\n",
        "    optimizer_dlg = torch.optim.LBFGS((dummy_data, dummy_label), max_iter=20)\n",
        "\n",
        "    global opt_steps; opt_steps = 0\n",
        "    for epoch in range(100):\n",
        "        # closure function needed for LBFGS optimizer\n",
        "        def closure():\n",
        "            global opt_steps; opt_steps += 1\n",
        "\n",
        "            # compute gradient of dummy data/label\n",
        "            optimizer_dlg.zero_grad()\n",
        "            pred = net(dummy_data)\n",
        "            dummy_onehot_label = F.softmax(dummy_label, dim=1)\n",
        "            dummy_loss = criterion(pred, dummy_onehot_label)\n",
        "            dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
        "            \n",
        "            # compute loss function, i.e. the SE of the gradients\n",
        "            grad_diff = 0\n",
        "            for gx, gy in zip(dummy_dy_dx, original_dy_dx):\n",
        "                grad_diff += ((gx - gy) ** 2).sum()\n",
        "            \n",
        "            grad_diff.backward()\n",
        "            return grad_diff\n",
        "        \n",
        "        # perform GD and log information\n",
        "        optimizer_dlg.step(closure)\n",
        "        current_loss = closure()\n",
        "        losses.append(current_loss.item())\n",
        "\n",
        "        if (verbose):\n",
        "            print(current_loss)\n",
        "        # if (current_loss < 1e-9):\n",
        "        #     break\n",
        "        # setting an upper limit on the number of optimization steps (e.g. limited attacking capability)\n",
        "        #if (opt_steps >= 80): \n",
        "        #    break\n",
        "    \n",
        "    return dummy_data, opt_steps, losses"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7i_ZnEbBavmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Batch-DLG with Adam\n",
        "# note: single-update optimization should not be used on one optimizer since Adam requires gradient history to perform updates --> see code block below\n",
        "# this appears to take longer to converge but may give better results than LBFGS on batches (without using optimization)\n",
        "\n",
        "# DLG algorithm on a given set of flowers and returns the hypothesized input with gradient losses\n",
        "def batch_DLG_Adam(net, criterion, device, original_dy_dx, batch_size, gt_data_len, gt_onehot_label_len, epoch_mult = 500, w_decay = 1e-12, scheduler_k = 0.001, verbose = False):\n",
        "    losses = []\n",
        "    opt_steps = epoch_mult * batch_size\n",
        "\n",
        "    # identify (data, label) using LBFGS on the squared difference between the original and guessed gradient\n",
        "    dummy_data = torch.randn(batch_size, gt_data_len).to(device).requires_grad_(True)\n",
        "    dummy_label = torch.randn(batch_size, gt_onehot_label_len).to(device).requires_grad_(True)\n",
        "    optimizer_dlg = torch.optim.Adam((dummy_data, dummy_label), lr=1, weight_decay=w_decay) # optimal learning rate seems to depend on the batch size of the dlg\n",
        "    scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer_dlg, lambda epoch: scheduler_k ** (1/opt_steps))\n",
        "\n",
        "    for epoch in trange(opt_steps):\n",
        "        optimizer_dlg.zero_grad()\n",
        "        pred = net(dummy_data)\n",
        "        dummy_onehot_label = F.softmax(dummy_label, dim=1)\n",
        "        dummy_loss = criterion(pred, dummy_onehot_label)\n",
        "        dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
        "        \n",
        "        # compute loss function, i.e. the SE of the gradients\n",
        "        grad_diff = 0\n",
        "        for gx, gy in zip(dummy_dy_dx, original_dy_dx):\n",
        "            grad_diff += ((gx - gy) ** 2).sum()\n",
        "\n",
        "        grad_diff.backward()\n",
        "\n",
        "        # Adam depends on past updates, and so this doesn't really work - loss fluctuates in dlg attack *significantly* when used\n",
        "        # only update a single dummy_data/dummy_label at a time\n",
        "        # mult = torch.zeros([batch_size, 1])\n",
        "        # mult[epoch%batch_size, 0] = 1\n",
        "        # dummy_data.grad *= mult; dummy_label.grad *= mult\n",
        "        \n",
        "        optimizer_dlg.step()\n",
        "        scheduler.step()\n",
        "        losses.append(grad_diff.item())\n",
        "\n",
        "        if verbose:\n",
        "            print(grad_diff)\n",
        "        # if (grad_diff < 1e-9):\n",
        "        #     break\n",
        "    \n",
        "    return dummy_data, losses\n",
        "\n",
        "print('defined: batch_DLG_Adam(net, criterion, device, original_dy_dx, batch_size, gt_data_len, gt_onehot_label_len, epoch_mult = 500, w_decay = 1e-12, scheduler_k = 0.001, verbose = False): dummy_data, losses')"
      ],
      "metadata": {
        "id": "EmwshMMOawXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d820660-177b-47e5-8731-e5f5d9ae4513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defined: batch_DLG_Adam(net, criterion, device, original_dy_dx, batch_size, gt_data_len, gt_onehot_label_len, epoch_mult = 500, w_decay = 1e-12, scheduler_k = 0.001, verbose = False): dummy_data, losses\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Batch-DLG with Adam - individual optimizers (not updated)\n",
        "# this doesn't seem to converge nearly as well as a single Adam optimizer\n",
        "\n",
        "# DLG algorithm on a given set of flowers and returns the hypothesized input\n",
        "def batch_DLG_Adam2(original_dy_dx, batch_size, gt_data_len, gt_onehot_label_len, verbose = False):\n",
        "    losses = []\n",
        "\n",
        "    dummy_data = [torch.randn(1, 4).to(device).requires_grad_(True) for i in range(batch_size)]\n",
        "    dummy_label = [torch.randn(1, 3).to(device).requires_grad_(True) for i in range(batch_size)]\n",
        "    optimizer_dlg = [torch.optim.Adam((dummy_data[i], dummy_label[i]), lr=1, weight_decay=1e-9) for i in range(batch_size)]\n",
        "    #scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer_dlg, lambda epoch: 0.999)\n",
        "\n",
        "    opt_steps = 500 * batch_size\n",
        "    for epoch in trange(opt_steps):\n",
        "        idx = epoch % batch_size\n",
        "        optimizer_dlg[idx].zero_grad()\n",
        "        pred = [net(dummy_data[i]) for i in range(batch_size)]\n",
        "        dummy_onehot_labels = [F.softmax(dummy_label[i], dim=1) for i in range(batch_size)]\n",
        "        dummy_loss = sum([criterion(pred[i], dummy_onehot_labels[i]) for i in range(batch_size)]) / batch_size\n",
        "        dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
        "        \n",
        "        # compute loss function, i.e. the SE of the gradients\n",
        "        grad_diff = 0\n",
        "        for gx, gy in zip(dummy_dy_dx, original_dy_dx):\n",
        "            grad_diff += ((gx - gy) ** 2).sum()\n",
        "\n",
        "        grad_diff.backward()\n",
        "        optimizer_dlg[idx].step()\n",
        "        #scheduler.step()\n",
        "        losses.append(grad_diff.item())\n",
        "\n",
        "        if verbose:\n",
        "            print(grad_diff)\n",
        "        # if (grad_diff < 1e-9):\n",
        "        #     break\n",
        "    \n",
        "    return dummy_data, opt_steps, losses"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rTgHemRXa3YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Batch-DLG with SGD (not updated)\n",
        "# this doesn't seem to converge nearly as well as LBFGS even with single-update optimization\n",
        "\n",
        "# DLG algorithm on a given set of flowers and returns the hypothesized input\n",
        "def batch_DLG_SGD(original_dy_dx, batch_size, gt_data_len, gt_onehot_label_len, verbose = False):\n",
        "    losses = []\n",
        "    opt_steps = 500 * batch_size\n",
        "\n",
        "    # identify (data, label) using LBFGS on the squared difference between the original and guessed gradient\n",
        "    dummy_data = torch.randn(batch_size, gt_data_len).to(device).requires_grad_(True)\n",
        "    dummy_label = torch.randn(batch_size, gt_onehot_label_len).to(device).requires_grad_(True)\n",
        "    optimizer_dlg = torch.optim.SGD((dummy_data, dummy_label), lr=1) #weight_decay=1e-9)#, momentum=0.001)#, weight_decay=1e-9)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer_dlg, lambda epoch: 0.001 ** (1/opt_steps))\n",
        "\n",
        "    for epoch in trange(opt_steps):\n",
        "        optimizer_dlg.zero_grad()\n",
        "        pred = net(dummy_data)\n",
        "        dummy_onehot_label = F.softmax(dummy_label, dim=1)\n",
        "        dummy_loss = criterion(pred, dummy_onehot_label)\n",
        "        dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
        "        \n",
        "        # compute loss function, i.e. the SE of the gradients\n",
        "        grad_diff = 0\n",
        "        for gx, gy in zip(dummy_dy_dx, original_dy_dx):\n",
        "            grad_diff += ((gx - gy) ** 2).sum()\n",
        "\n",
        "        grad_diff.backward()\n",
        "\n",
        "        # only update a single dummy_data/dummy_label at a time\n",
        "        mult = torch.zeros([batch_size, 1])\n",
        "        mult[epoch%batch_size, 0] = 1\n",
        "        dummy_data.grad *= mult; dummy_label.grad *= mult\n",
        "        \n",
        "        optimizer_dlg.step()\n",
        "        scheduler.step()\n",
        "        losses.append(grad_diff.item())\n",
        "\n",
        "        if verbose:\n",
        "            print(grad_diff)\n",
        "        # if (grad_diff < 1e-9):\n",
        "        #     break\n",
        "    \n",
        "    return dummy_data, opt_steps, losses"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7BlG6Mqha-_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "XRvhEO4Tbrbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# auxiliary functions for NN - convert to onehot and loss function\n",
        "def label_to_onehot(target, n_classes):\n",
        "    onehot_target = torch.zeros(target.size(0), n_classes, device=target.device)\n",
        "    onehot_target.scatter_(1, target, 1)\n",
        "    return onehot_target\n",
        "\n",
        "print('defined: label_to_onehot(target, n_classes): onehot_target')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cg_sT1wTeHO",
        "outputId": "06d29e0a-640c-4c8c-cdb0-f6a0089bb492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defined: label_to_onehot(target, n_classes): onehot_target\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train NN without running batch dlg\n",
        "# Note: converges sometimes to near-optimal predictions\n",
        "def train_net_Adam(net, criterion, device, train_data, train_target, output_dim, batch_size = 32, epochs = 100):\n",
        "    losses = []\n",
        "    train_dst_len = train_data.shape[0]\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-5) # regularizer may not be necessary\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rand_subset = list(RandomSampler(range(train_dst_len), num_samples=batch_size))\n",
        "        gt_data = torch.tensor(train_data[rand_subset]).to(device)\n",
        "        gt_label = torch.tensor(train_target[rand_subset]).to(device)\n",
        "        gt_onehot_label = label_to_onehot(gt_label, n_classes = output_dim)\n",
        "\n",
        "        output = net(gt_data.float())\n",
        "        loss = criterion(output, gt_onehot_label)\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        #batch_accuracy = torch.sum(torch.eq(torch.argmax(output, dim=1), gt_label)) / batch_size\n",
        "        #accuracies.append(batch_accuracy)\n",
        "        losses.append(loss.detach().clone())\n",
        "    \n",
        "    return losses\n",
        "\n",
        "print('defined: train_net_Adam(net, criterion, device, train_data, train_target, output_dim, batch_size = 32, epochs = 100): losses')"
      ],
      "metadata": {
        "id": "xJlCndWTcfKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "037f87c1-f541-4c69-979d-3fb51cd5d34d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defined: train_net_Adam(net, criterion, device, train_data, train_target, output_dim, batch_size = 32, epochs = 100): losses\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_net(net, test_data, test_target):\n",
        "    test_dst_len = test_data.shape[0]\n",
        "    pred = net(torch.tensor(test_data).float())\n",
        "    correct = torch.sum(torch.eq(torch.argmax(pred, dim=1, keepdim=True), torch.tensor(test_target)))\n",
        "    print(f'score: {correct}/{test_dst_len}')\n",
        "\n",
        "print('defined: test_net(net, test_data, test_target): prints accuracy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QokAUUhkC037",
        "outputId": "bc67e41c-ffbf-4c89-8d23-3703562d9b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defined: test_net(net, test_data, test_target): prints accuracy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title train with LBFGS optimizer\n",
        "\n",
        "# NOTE: LBFGS isn't converging when using mini-batches\n",
        "def train_net_LBFGS(train_data, train_target, batch_size = 16, epochs = 100):\n",
        "    print(train_data); print(train_target)\n",
        "    train_dst_len = train_data.shape[0]\n",
        "    optimizer = torch.optim.LBFGS(net.parameters(), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lambda epoch: 0.99)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        rand_subset = list(RandomSampler(range(train_dst_len), num_samples=batch_size))\n",
        "        # print('epoch, randset: ', epoch, rand_subset)\n",
        "        gt_data = torch.tensor(train_data[rand_subset]).to(device)\n",
        "        gt_label = torch.tensor(train_target[rand_subset]).to(device)\n",
        "        gt_onehot_label = label_to_onehot(gt_label, num_classes=3)\n",
        "        #print('lbfgs: ', gt_data, gt_onehot_label)\n",
        "        \n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            output = net(gt_data.float())\n",
        "            loss = criterion(output, gt_onehot_label)\n",
        "            loss.backward()\n",
        "            #print(f'output: {output}, onehot_label: {gt_onehot_label}')\n",
        "            print('loss: ', loss)\n",
        "            return loss\n",
        "      \n",
        "        optimizer.step(closure)\n",
        "        scheduler.step()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0drLMq-cc9gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: this function \n",
        "def train_net_Adam_DLG(net, criterion, device, scaler, train_data, train_target, output_dim, batch_size = 32, epochs = 100, verbose = False, dlg_rate = 10, gt_data_len = -1, gt_onehot_label_len = -1, epoch_mult=100, w_decay=1e-12, scheduler_k = 0.01, leak_no_train = False):\n",
        "    train_dst_len = train_data.shape[0]\n",
        "    optimizer = torch.optim.Adam(net.parameters()) #lr=0.001)\n",
        "\n",
        "    # statistics\n",
        "    losses = []; grad_norms = []\n",
        "    dlg_timestamps = []\n",
        "    AA_REs = []; CA_REs = []\n",
        "    AA_RRate = []; CA_RRate = []\n",
        "    AA_cos_angles = []; CA_cos_angles = []\n",
        "\n",
        "    for epoch in trange(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rand_subset = list(RandomSampler(range(train_dst_len), num_samples=batch_size))\n",
        "        # print('epoch, randset: ', epoch, rand_subset)\n",
        "        gt_data = torch.tensor(train_data[rand_subset]).to(device)\n",
        "        gt_label = torch.tensor(train_target[rand_subset]).to(device)\n",
        "        gt_onehot_label = label_to_onehot(gt_label, n_classes= output_dim)\n",
        "\n",
        "        output = net(gt_data.float())\n",
        "        #print('adam: ', output.tolist(), gt_onehot_label.tolist())\n",
        "        loss = criterion(output, gt_onehot_label)\n",
        "\n",
        "        # Perform DLG ---------------------------------------------------------------\n",
        "\n",
        "        if (epoch%dlg_rate == 0):\n",
        "            batch_dy_dx = torch.autograd.grad(loss, net.parameters(), retain_graph = True)\n",
        "            batch_norm = sqrt(sum([torch.linalg.norm(_.detach())**2 for _ in batch_dy_dx]))\n",
        "            grad_norms.append(batch_norm)\n",
        "            original_dy_dx = list((_.detach().clone() for _ in batch_dy_dx)) # share the gradients with other clients\n",
        "            guess, _ = batch_DLG_Adam(net, criterion, device, original_dy_dx, batch_size, gt_data_len, gt_onehot_label_len, epoch_mult, w_decay=1e-12, scheduler_k = 0.01, verbose=False)\n",
        "            \n",
        "            # rescale data for analysis\n",
        "            gt_data = torch.tensor(scaler.inverse_transform(gt_data.detach().clone()))\n",
        "            guess = torch.tensor(scaler.inverse_transform(guess.detach().clone()))\n",
        "            \n",
        "            # assign all\n",
        "            guess_perm = assign_guess(guess, gt_data, batch_size, verbose)\n",
        "            rel_errors, recovered_rate, cos_angles = compute_stats(guess_perm, gt_data)\n",
        "            AA_REs.append(rel_errors)\n",
        "            AA_RRate.append(recovered_rate)\n",
        "            AA_cos_angles.append(cos_angles)\n",
        "            dlg_timestamps.append(epoch)\n",
        "\n",
        "            # closest assignment\n",
        "            best_match = assign_best(guess, gt_data, batch_size, verbose)\n",
        "            rel_errors, recovered_rate, cos_angles = compute_stats(guess, best_match)\n",
        "            CA_REs.append(rel_errors)\n",
        "            CA_RRate.append(recovered_rate)\n",
        "            CA_cos_angles.append(cos_angles)\n",
        "\n",
        "        # end of DLG ----------------------------------------------------------------\n",
        "\n",
        "        # issue: running batch_DLG seems to ruin the stored gradients of the parameters\n",
        "        # consider creating a separate ml model and running dlg on that\n",
        "        # inefficient - REMOVE -----------------------------------------------------------------------------\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, gt_onehot_label)\n",
        "        # end of REMOVE ------------------------------------------------------------------------------------\n",
        "\n",
        "        losses.append(loss.detach().clone())\n",
        "        # *** if leak_no_train is set to true, then don't update model (e.g. for leaking on random weights) ***\n",
        "        if (leak_no_train):\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "        if (verbose):\n",
        "            print('current loss: ', loss)\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        #batch_accuracy = torch.sum(torch.eq(torch.argmax(output, dim=1), gt_label)) / batch_size\n",
        "        #accuracies.append(batch_accuracy)\n",
        "        \n",
        "    \n",
        "    return losses, dlg_timestamps, AA_REs, AA_RRate , AA_cos_angles, CA_REs, CA_RRate, CA_cos_angles, grad_norms\n",
        "  \n",
        "print('defined: train_net_Adam_DLG(net, criterion, device, scaler, train_data, train_target, output_dim, batch_size = 32, epochs = 100, verbose = False, dlg_rate = 10, gt_data_len = -1, gt_onehot_label_len = -1, epoch_mult=100, w_decay=1e-12, scheduler_k = 0.01, leak_no_train = False): losses, dlg_timestamps, AA_REs, AA_RRate , AA_cos_angles, CA_REs, CA_RRate, CA_cos_angles')\n",
        "print(' --> leak_no_train allows for running DLG on the same set of random weights for statistical purposes')"
      ],
      "metadata": {
        "id": "uE-zEN9Ubpch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a48bc2-8dee-4f78-c9eb-2fed5a3516d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defined: train_net_Adam_DLG(net, criterion, device, scaler, train_data, train_target, output_dim, batch_size = 32, epochs = 100, verbose = False, dlg_rate = 10, gt_data_len = -1, gt_onehot_label_len = -1, epoch_mult=100, w_decay=1e-12, scheduler_k = 0.01, leak_no_train = False): losses, dlg_timestamps, AA_REs, AA_RRate , AA_cos_angles, CA_REs, CA_RRate, CA_cos_angles\n",
            " --> leak_no_train allows for running DLG on the same set of random weights for statistical purposes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WLD_sN6tVb2P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}